"""
template_analysis.py
────────────────────
Read two JSONL files produced by *template_siliconsampling.py* (or similar),
compute descriptive statistics, perform a Welch *t*-test, approximate the
Bayes Factor (BF₁₀) via a BIC method, and visualise the distributions.

Author: auto‑generated by ChatGPT (July 2025)
"""

from __future__ import annotations

import json
import math
import statistics
import csv
from collections import Counter
from pathlib import Path

ROOT_DIR = Path(__file__).resolve().parents[2]   # …/project-LLMs
OUTPUT_DIR = ROOT_DIR / "project-Silicon_Sampling" / "llm_outputs"

import matplotlib.pyplot as plt
from matplotlib.ticker import MaxNLocator
from scipy.stats import t as _t_dist

# ──────────────────────────────────────────────────────────────
# Configuration
# ──────────────────────────────────────────────────────────────
FILE_A = OUTPUT_DIR / "output_memory_1.jsonl"       # Condition A
FILE_B = OUTPUT_DIR / "output_memory_2.jsonl"       # Condition B
SLOT_INDEX = 0                                           # which numeric slot to extract (0 = first number found)

KEY_NAME = "depth"                                       # numeric key in response_json dict
CONDITION_A_NAME = "Control"                             # human‑readable label for A
CONDITION_B_NAME = "Treatment"                           # human‑readable label for B
PLOT_TITLE = "Distributions of Silicon Sampling Scores"
FIG_PATH = OUTPUT_DIR / "analysis_silicon_sampling_distributions.png"

# Optional output: 
PRINT_RAW_VALUES = False                                            # Toggle to display the raw lists
CSV_PATH = OUTPUT_DIR / "data-silicon_sampling_values.csv"     # CSV output file
WRITE_CSV = False                                                   # toggle CSV export


# ──────────────────────────────────────────────────────────────
# Helper functions
# ──────────────────────────────────────────────────────────────
def _extract_json_payload(text: str) -> str:
    """
    Helper copied from functions_parallel_calls.py: isolate JSON part of *text*
    (handles ```json fences, curly quotes, leading noise, etc.).
    """
    import re

    text = text.strip()
    if text.startswith("```"):
        m = re.match(r"```(?:json)?\s*(.*?)\s*```", text, re.DOTALL)
        if m:
            text = m.group(1).strip()

    start, end = text.find("{"), text.rfind("}")
    if start != -1 and end != -1:
        text = text[start : end + 1]

    text = (
        text.replace("“", '"')
        .replace("”", '"')
        .replace("‘", "'")
        .replace("’", "'")
    )
    return text


def _safe_stdev(seq: list[float]) -> float:
    """Return statistics.stdev(seq) if len(seq) ≥ 2, else float('nan')."""
    return statistics.stdev(seq) if len(seq) >= 2 else float("nan")


def _read_slot_values(path: Path, slot_idx: int = 0, key_name: str | None = None) -> list[float]:
    """
    Return a list of numeric values extracted from *path*.

    Extraction rules (in priority order):
    1. If *key_name* is provided and `response_json` is a dict containing that
       key, use that numeric value.
    2. Else, if `response_json` is a dict, take the **first** numeric value.
    3. Else, if `response_json` is a list, take index *slot_idx* if numeric.
    4. Else, if `response_json` is a JSON‑serialised string, attempt to parse
       and then apply the rules above.
    """
    values: list[float] = []

    with path.open(encoding="utf-8") as fh:
        for line in fh:
            if not line.strip():
                continue  # skip blank lines
            try:
                rec = json.loads(line)
            except json.JSONDecodeError:
                continue  # malformed record – skip
            payload = rec.get("response_json")

            # If payload is a string, attempt to load JSON inside it
            if isinstance(payload, str):
                cleaned = _extract_json_payload(payload)
                try:
                    payload = json.loads(cleaned)
                except json.JSONDecodeError:
                    continue  # not valid JSON → skip

            # Case 1 & 2: dict
            if isinstance(payload, dict):
                if key_name and key_name in payload and isinstance(payload[key_name], (int, float)):
                    values.append(float(payload[key_name]))
                else:
                    for v in payload.values():
                        if isinstance(v, (int, float)):
                            values.append(float(v))
                            break

            # Case 3: list
            elif isinstance(payload, list):
                if len(payload) > slot_idx and isinstance(payload[slot_idx], (int, float)):
                    values.append(float(payload[slot_idx]))

    return values


def _welch_t(mean_a: float, mean_b: float, sd_a: float, sd_b: float, n_a: int, n_b: int):
    """Return t‑statistic, degrees of freedom, p‑value (two‑tailed)."""
    # Guard against insufficient sample size
    if n_a <= 1 or n_b <= 1:
        return float("nan"), float("nan"), float("nan")

    se = math.sqrt(sd_a**2 / n_a + sd_b**2 / n_b)
    t_stat = (mean_a - mean_b) / se

    num = (sd_a**2 / n_a + sd_b**2 / n_b) ** 2
    denom = ((sd_a**2 / n_a) ** 2) / (n_a - 1) + ((sd_b**2 / n_b) ** 2) / (n_b - 1)
    df = num / denom

    p_val = 2 * (1 - _t_dist.cdf(abs(t_stat), df))
    return t_stat, df, p_val


def _bic_bayes_factor(t_stat: float, df: float, n_total: int) -> float:
    """
    Approximate BF₁₀ from Welch t using Wagenmakers (2007) BIC method.
    ΔBIC = n * ln(1 + t²/df) ;  BF₁₀ = exp(ΔBIC / 2).
    """
    delta_bic = n_total * math.log1p(t_stat**2 / df)
    log_bf = delta_bic / 2.0          # natural‑log Bayes factor

    # Avoid overflow: math.exp(x) overflows for x ≳ 709.
    if log_bf > 700:
        return float("inf")
    return math.exp(log_bf)


def _cohen_d(mean_a: float, mean_b: float, sd_a: float, sd_b: float, n_a: int, n_b: int) -> float:
    """
    Compute Cohen’s d for two independent samples.

    Uses the pooled (unbiased) standard deviation:
    SD_pooled = sqrt( ((n_a‑1)*sd_a² + (n_b‑1)*sd_b²) / (n_a + n_b – 2) )
    Returns signed *d* (positive if B > A).
    """
    pooled_sd = math.sqrt(((n_a - 1) * sd_a ** 2 + (n_b - 1) * sd_b ** 2) / (n_a + n_b - 2))
    return (mean_b - mean_a) / pooled_sd if pooled_sd else float("nan")

def _ci_cohen_d(d: float, n_a: int, n_b: int, confidence: float = 0.95) -> tuple[float, float]:
    """
    Return a confidence interval for Cohen's d using the Hedges & Olkin (1985)
    normal‑approximation standard error:

        SE_d = sqrt( (n_a + n_b) / (n_a * n_b) + d² / (2 * (n_a + n_b - 2)) )

    The CI is d ± z_crit · SE_d, where z_crit is the two‑tailed normal
    critical value (e.g., 1.96 for 95 %, 2.58 for 99 %).
    """
    se_d = math.sqrt((n_a + n_b) / (n_a * n_b) + (d ** 2) / (2 * (n_a + n_b - 2)))
    alpha = 1.0 - confidence
    z_crit = _t_dist.ppf(1 - alpha / 2, df=float("inf"))  # ≈ normal
    margin = z_crit * se_d
    return d - margin, d + margin


def _confidence_interval(
    mean_a: float,
    mean_b: float,
    sd_a: float,
    sd_b: float,
    n_a: int,
    n_b: int,
    df: float,
    confidence: float = 0.95,
) -> tuple[float, float]:
    """
    Return the CI for the *difference of means* (B − A) at the requested confidence level
    using Welch's SE and the corresponding df.
    """
    se = math.sqrt(sd_a**2 / n_a + sd_b**2 / n_b)
    alpha = 1.0 - confidence
    tcrit = _t_dist.ppf(1 - alpha / 2, df)
    diff = mean_b - mean_a
    margin = tcrit * se
    return diff - margin, diff + margin


# ──────────────────────────────────────────────────────────────
# Data extraction
# ──────────────────────────────────────────────────────────────
vals_a = _read_slot_values(FILE_A, SLOT_INDEX, KEY_NAME)
vals_b = _read_slot_values(FILE_B, SLOT_INDEX, KEY_NAME)

if PRINT_RAW_VALUES:
    print("\n--- Raw numeric values ---")
    print(f"A ({FILE_A.name}): {vals_a}")
    print(f"B ({FILE_B.name}): {vals_b}")

if not vals_a or not vals_b:
    raise RuntimeError("No numeric values extracted – check JSONL structure.")

# Stats for A
n_a = len(vals_a)
mean_a = statistics.mean(vals_a)
sd_a = _safe_stdev(vals_a)
min_a, max_a, med_a = min(vals_a), max(vals_a), statistics.median(vals_a)

# Stats for B
n_b = len(vals_b)
mean_b = statistics.mean(vals_b)
sd_b = _safe_stdev(vals_b)
min_b, max_b, med_b = min(vals_b), max(vals_b), statistics.median(vals_b)

# Welch t‑test & Bayes Factor
t_stat, df, p_val = _welch_t(mean_a, mean_b, sd_a, sd_b, n_a, n_b)
bf10 = _bic_bayes_factor(t_stat, df, n_a + n_b)
delta_bic = (n_a + n_b) * math.log1p(t_stat**2 / df)
log10_bf  = delta_bic / (2 * math.log(10))    # base‑10 Bayes factor
cohen_d = _cohen_d(mean_a, mean_b, sd_a, sd_b, n_a, n_b)
ci_d_95 = _ci_cohen_d(cohen_d, n_a, n_b, confidence=0.95)
ci_d_99 = _ci_cohen_d(cohen_d, n_a, n_b, confidence=0.99)

# ──────────────────────────────────────────────────────────────
# Report
# ──────────────────────────────────────────────────────────────
def _fmt(x: float) -> str:
    if math.isnan(x):
        return "nan"
    if math.isinf(x):
        return "inf"
    return f"{x:.3e}"


print("\n=== Descriptive statistics ===")
print(f"{CONDITION_A_NAME} ({FILE_A.name}) – n={n_a}")
print(f"  Mean={_fmt(mean_a)}  SD={_fmt(sd_a)}  Median={_fmt(med_a)}  Min={_fmt(min_a)}  Max={_fmt(max_a)}")

print(f"{CONDITION_B_NAME} ({FILE_B.name}) – n={n_b}")
print(f"  Mean={_fmt(mean_b)}  SD={_fmt(sd_b)}  Median={_fmt(med_b)}  Min={_fmt(min_b)}  Max={_fmt(max_b)}")

print("\n=== Inferential statistics ===")
print(f"Welch t({df:.1f}) = {t_stat:.3f}")
# Format p‑value; underflow (0.0) is shown as < 1 × 10⁻³⁰⁸
p_text = f"{p_val:.3e}" if p_val > 0 else "< 1 × 10⁻³⁰⁸"
print(f"\nTwo‑tailed p-value = {p_text}")
if p_val == 0.0:
    print("Note: The exact p-value is smaller than the minimum positive number "
          "representable in double‑precision (≈ 1 × 10⁻³⁰⁸). Report it as "
          "p < 1 × 10⁻³⁰⁸.")
if math.isinf(bf10):
    print(f"\nBayes Factor BF₁₀  >  1.0e308  (double‑precision limit)")
    print(f"log10(BF₁₀) ≈ {log10_bf:.1f}   [BIC approximation]")
    print("Interpretation: log10(BF₁₀) above ~2 is considered ‘decisive’; "
          "a value near 300 represents overwhelming evidence for the "
          "alternative model. In a manuscript, report for example:\n")
    manuscript_sentence = (
        f"Welch’s t-test indicated a substantial difference between conditions, "
        f"t({df:.1f}) = {t_stat:.2f}, p < .001. "
        f"The BIC-approximated Bayes factor exceeded the double-precision limit "
        f"(BF₁₀ > 1 × 10³⁰⁸), corresponding to log₁₀ BF₁₀ ≈ {log10_bf:.1f}, and "
        f"therefore provides overwhelming evidence for the alternative hypothesis "
        f"(cf. Kass & Raftery, 1995)."
    )
    print(manuscript_sentence)
else:
    print(f"Bayes Factor BF₁₀ ≈ {_fmt(bf10)}  "
          f"(log10 BF₁₀ = {log10_bf:.1f})")
print(f"\nCohen's d = {_fmt(cohen_d)}")
print(f"95% CI for Cohen's d: [{_fmt(ci_d_95[0])}, {_fmt(ci_d_95[1])}]")
print(f"99% CI for Cohen's d: [{_fmt(ci_d_99[0])}, {_fmt(ci_d_99[1])}]")

# ──────────────────────────────────────────────────────────────
# Visualisation
# ──────────────────────────────────────────────────────────────
# Create discrete histogram (bar chart) for each integer value in global range
global_min = int(min(min_a, min_b))
global_max = int(max(max_a, max_b))
x_vals = list(range(global_min, global_max + 1))

count_a = Counter(int(round(v)) for v in vals_a)
count_b = Counter(int(round(v)) for v in vals_b)

heights_a = [count_a.get(x, 0) for x in x_vals]
heights_b = [count_b.get(x, 0) for x in x_vals]

width = 0.4
offset = width / 2

fig, ax = plt.subplots()
ax.bar([x - offset for x in x_vals], heights_a, width=width, label=CONDITION_A_NAME)
ax.bar([x + offset for x in x_vals], heights_b, width=width, label=CONDITION_B_NAME)

ax.set_xlabel("Score (integer rounded)")
ax.set_ylabel("Frequency")
ax.set_title(PLOT_TITLE)
ax.legend()
ax.xaxis.set_major_locator(MaxNLocator(integer=True))

plt.tight_layout()
plt.savefig(FIG_PATH, dpi=300)
print(f"\nFigure saved to {FIG_PATH.resolve()}")

# ──────────────────────────────────────────────────────────────
# Optional CSV export of raw values
# ──────────────────────────────────────────────────────────────
if WRITE_CSV:
    max_len = max(len(vals_a), len(vals_b))
    pad_a = vals_a + [""] * (max_len - len(vals_a))
    pad_b = vals_b + [""] * (max_len - len(vals_b))

    with CSV_PATH.open("w", newline="", encoding="utf-8") as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow([CONDITION_A_NAME, CONDITION_B_NAME])
        writer.writerows(zip(pad_a, pad_b))

    print(f"CSV file saved to {CSV_PATH.resolve()}")
