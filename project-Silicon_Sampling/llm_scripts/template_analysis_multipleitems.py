"""
template_analysis_multipleitems.py
──────────────────────────────────
Analyse two JSONL files (conditions A and B) whose LLM responses contain
multiple numeric items (e.g., {"item1": 9, "item2": 4, "item3": 8}).

For each record:
1. Select the item keys listed in ITEM_KEYS.
2. Reverse‑score any key listed in REVERSED_KEYS using
   rev(x) = (LIKERT_MIN + LIKERT_MAX) − x.
3. Compute the mean of those items → one value per response.

Then compute:
• Descriptive stats
• Welch t‑test
• Bayes Factor (BIC)
• Cohen’s d with 95 % and 99 % CIs
• Visual histogram
• Optional CSV export

Author: auto‑generated by ChatGPT (July 2025)
"""

from __future__ import annotations

import json
import math
import statistics
import csv
from collections import Counter
from pathlib import Path

import matplotlib.pyplot as plt
from matplotlib.ticker import MaxNLocator
from scipy.stats import t as _t_dist

# ──────────────────────────────────────────────────────────────
# Paths
# ──────────────────────────────────────────────────────────────
ROOT_DIR   = Path(__file__).resolve().parents[2]                   # …/project-LLMs
OUTPUT_DIR = ROOT_DIR / "project-Silicon_Sampling" / "llm_outputs"

FILE_A = OUTPUT_DIR / "output_memory_1.jsonl"                      # Condition A
FILE_B = OUTPUT_DIR / "output_memory_2.jsonl"                      # Condition B

# ──────────────────────────────────────────────────────────────
# Configuration – EDIT HERE
# ──────────────────────────────────────────────────────────────
# Keys to average in each response
ITEM_KEYS     = ["item1", "item2", "item3"]      # The script is indifferent to the number of items

# Keys that must be reverse‑scored (subset of ITEM_KEYS)
REVERSED_KEYS = ["item2"]

# Rating scale endpoints (inclusive) used by the LLM (necessary for reverse-scoring)
LIKERT_MIN = 1
LIKERT_MAX = 7

CONDITION_A_NAME = "Control"
CONDITION_B_NAME = "Treatment"
PLOT_TITLE       = "Distributions of Mean Item Scores"
FIG_PATH         = OUTPUT_DIR / "analysis_silicon_sampling_distributions.png"

PRINT_RAW_VALUES = False                # Print extracted per‑response means
CSV_PATH         = OUTPUT_DIR / "data-silicon_sampling_values.csv"
WRITE_CSV        = False                # Export CSV of raw means

# ──────────────────────────────────────────────────────────────
# Helpers
# ──────────────────────────────────────────────────────────────
def _extract_json_payload(text: str) -> str:
    """Isolate JSON inside fenced block or noisy string."""
    import re
    text = text.strip()
    if text.startswith("```"):
        m = re.match(r"```(?:json)?\s*(.*?)\s*```", text, re.DOTALL)
        if m:
            text = m.group(1).strip()
    start, end = text.find("{"), text.rfind("}")
    if start != -1 and end != -1:
        text = text[start : end + 1]
    return (
        text.replace("“", '"')
        .replace("”", '"')
        .replace("‘", "'")
        .replace("’", "'")
    )

def _safe_stdev(seq: list[float]) -> float:
    return statistics.stdev(seq) if len(seq) >= 2 else float("nan")

def _read_response_means(
    path: Path,
    item_keys: list[str],
    reversed_keys: set[str],
    scale_min: float,
    scale_max: float,
) -> list[float]:
    """Return per‑response mean of specified items (reverse‑scoring handled)."""
    means: list[float] = []

    with path.open(encoding="utf-8") as fh:
        for line in fh:
            if not line.strip():
                continue
            try:
                rec = json.loads(line)
            except json.JSONDecodeError:
                continue
            payload = rec.get("response_json")

            # Decode string‑encoded JSON if needed
            if isinstance(payload, str):
                try:
                    payload = json.loads(_extract_json_payload(payload))
                except json.JSONDecodeError:
                    continue

            values: list[float] = []

            if isinstance(payload, dict):
                try:
                    for k in item_keys:
                        v = payload[k]
                        if not isinstance(v, (int, float)):
                            raise KeyError
                        if k in reversed_keys:
                            v = (scale_min + scale_max) - float(v)
                        values.append(float(v))
                except (KeyError, TypeError):
                    continue  # missing or non‑numeric → skip

            elif isinstance(payload, list):
                if len(payload) < len(item_keys):
                    continue
                for idx, k in enumerate(item_keys):
                    v = payload[idx]
                    if not isinstance(v, (int, float)):
                        values = []
                        break
                    if k in reversed_keys:
                        v = (scale_min + scale_max) - float(v)
                    values.append(float(v))

            if len(values) == len(item_keys):
                means.append(statistics.mean(values))

    return means

def _welch_t(mean_a, mean_b, sd_a, sd_b, n_a, n_b):
    se = math.sqrt(sd_a**2 / n_a + sd_b**2 / n_b)
    t  = (mean_a - mean_b) / se
    num = (sd_a**2 / n_a + sd_b**2 / n_b) ** 2
    den = ((sd_a**2 / n_a) ** 2) / (n_a - 1) + ((sd_b**2 / n_b) ** 2) / (n_b - 1)
    df  = num / den
    p   = 2 * (1 - _t_dist.cdf(abs(t), df))
    return t, df, p

def _bic_bayes_factor(t, df, n):
    """
    Wagenmakers (2007) BIC approximation:
    ΔBIC = n * ln(1 + t²/df)  →  BF₁₀ = exp(ΔBIC / 2).
    Guard against overflow when exp(argument) > ~709.
    """
    delta_bic = n * math.log1p(t ** 2 / df)
    log_bf = delta_bic / 2.0           # natural‑log Bayes factor
    if log_bf > 700:                   # double‑precision limit
        return float("inf")
    return math.exp(log_bf)

def _cohen_d(mean_a, mean_b, sd_a, sd_b, n_a, n_b):
    pooled = math.sqrt(((n_a - 1) * sd_a ** 2 + (n_b - 1) * sd_b ** 2) / (n_a + n_b - 2))
    return (mean_b - mean_a) / pooled if pooled else float("nan")

def _ci_cohen_d(d, n_a, n_b, confidence=0.95):
    se = math.sqrt((n_a + n_b) / (n_a * n_b) + d ** 2 / (2 * (n_a + n_b - 2)))
    z  = _t_dist.ppf(1 - (1 - confidence) / 2, df=float("inf"))
    m  = z * se
    return d - m, d + m

def _fmt(x: float) -> str:
    if math.isnan(x):
        return "nan"
    if math.isinf(x):
        return "inf"
    return f"{x:.3e}"

# ──────────────────────────────────────────────────────────────
# Data extraction
# ──────────────────────────────────────────────────────────────
vals_a = _read_response_means(FILE_A, ITEM_KEYS, set(REVERSED_KEYS), LIKERT_MIN, LIKERT_MAX)
vals_b = _read_response_means(FILE_B, ITEM_KEYS, set(REVERSED_KEYS), LIKERT_MIN, LIKERT_MAX)

if PRINT_RAW_VALUES:
    print("\n--- Per‑response means ---")
    print(f"A: {vals_a}")
    print(f"B: {vals_b}")

if not vals_a or not vals_b:
    raise RuntimeError("No numeric values extracted – check ITEM_KEYS or JSONL structure.")

# Descriptive statistics
n_a, n_b = len(vals_a), len(vals_b)
mean_a, mean_b = statistics.mean(vals_a), statistics.mean(vals_b)
sd_a, sd_b     = _safe_stdev(vals_a), _safe_stdev(vals_b)
min_a, max_a, med_a = min(vals_a), max(vals_a), statistics.median(vals_a)
min_b, max_b, med_b = min(vals_b), max(vals_b), statistics.median(vals_b)

# Inferential statistics
t_stat, df, p_val = _welch_t(mean_a, mean_b, sd_a, sd_b, n_a, n_b)
bf10              = _bic_bayes_factor(t_stat, df, n_a + n_b)
delta_bic = (n_a + n_b) * math.log1p(t_stat**2 / df)
log10_bf  = delta_bic / (2 * math.log(10))     # base‑10 Bayes factor
d                 = _cohen_d(mean_a, mean_b, sd_a, sd_b, n_a, n_b)
d_ci95            = _ci_cohen_d(d, n_a, n_b, 0.95)
d_ci99            = _ci_cohen_d(d, n_a, n_b, 0.99)

# ──────────────────────────────────────────────────────────────
# Report
# ──────────────────────────────────────────────────────────────
print("\n=== Descriptive statistics ===")
print(f"{CONDITION_A_NAME} – n={n_a}  Mean={_fmt(mean_a)}  SD={_fmt(sd_a)}  "
      f"Median={_fmt(med_a)}  Min={_fmt(min_a)}  Max={_fmt(max_a)}")
print(f"{CONDITION_B_NAME} – n={n_b}  Mean={_fmt(mean_b)}  SD={_fmt(sd_b)}  "
      f"Median={_fmt(med_b)}  Min={_fmt(min_b)}  Max={_fmt(max_b)}")

print("\n=== Inferential statistics ===")
print(f"Welch t({df:.1f}) = {t_stat:.3f}")
# Format p‑value; underflow (0.0) is shown as < 1 × 10⁻³⁰⁸
p_text = f"{p_val:.3e}" if p_val > 0 else "< 1 × 10⁻³⁰⁸"
print(f"\nTwo‑tailed p‑value = {p_text}")
if p_val == 0.0:
    print("Note: The exact p‑value is smaller than the minimum positive number "
          "representable in double‑precision (≈ 1 × 10⁻³⁰⁸). Report it as "
          "p < 1 × 10⁻³⁰⁸.")

if math.isinf(bf10):
    print(f"\nBayes Factor BF₁₀  >  1.0e308  (double‑precision limit)")
    print(f"log10(BF₁₀) ≈ {log10_bf:.1f}   [BIC approximation]")
    print("Interpretation: log10(BF₁₀) above ~2 is considered decisive; "
          "a value near 300 represents overwhelming evidence for the "
          "alternative model.")
    manuscript_sentence = (
        f"Welch’s t-test indicated a substantial difference between conditions, "
        f"t({df:.1f}) = {t_stat:.2f}, p < .001. "
        f"The BIC‑approximated Bayes factor exceeded the double‑precision limit "
        f"(BF₁₀ > 1 × 10³⁰⁸), corresponding to log₁₀ BF₁₀ ≈ {log10_bf:.1f}, and "
        f"therefore provides overwhelming evidence for the alternative hypothesis "
        f"(cf. Kass & Raftery, 1995)."
    )
    print("\nSuggested manuscript wording:\n" + manuscript_sentence)
else:
    print(f"Bayes Factor BF₁₀ ≈ {_fmt(bf10)}  "
          f"(log10 BF₁₀ = {log10_bf:.1f})")

print(f"\nCohen's d = {_fmt(d)}")
print(f"95% CI for d: [{_fmt(d_ci95[0])}, {_fmt(d_ci95[1])}]")
print(f"99% CI for d: [{_fmt(d_ci99[0])}, {_fmt(d_ci99[1])}]")

# ──────────────────────────────────────────────────────────────
# Visualisation
# ──────────────────────────────────────────────────────────────
global_min = int(math.floor(min(min_a, min_b)))
global_max = int(math.ceil(max(max_a, max_b)))
x_vals = list(range(global_min, global_max + 1))

count_a = Counter(int(round(v)) for v in vals_a)
count_b = Counter(int(round(v)) for v in vals_b)
heights_a = [count_a.get(x, 0) for x in x_vals]
heights_b = [count_b.get(x, 0) for x in x_vals]

fig, ax = plt.subplots()
width, offset = 0.4, 0.2
ax.bar([x - offset for x in x_vals], heights_a, width=width, label=CONDITION_A_NAME)
ax.bar([x + offset for x in x_vals], heights_b, width=width, label=CONDITION_B_NAME)

ax.set_xlabel("Mean score (rounded)")
ax.set_ylabel("Frequency")
ax.set_title(PLOT_TITLE)
ax.legend()
ax.xaxis.set_major_locator(MaxNLocator(integer=True))

plt.tight_layout()
plt.savefig(FIG_PATH, dpi=300)
print(f"\nFigure saved to {FIG_PATH.resolve()}")

# ──────────────────────────────────────────────────────────────
# Optional CSV export of raw means
# ──────────────────────────────────────────────────────────────
if WRITE_CSV:
    max_len = max(len(vals_a), len(vals_b))
    pad_a = vals_a + [""] * (max_len - len(vals_a))
    pad_b = vals_b + [""] * (max_len - len(vals_b))

    with CSV_PATH.open("w", newline="", encoding="utf-8") as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow([CONDITION_A_NAME, CONDITION_B_NAME])
        writer.writerows(zip(pad_a, pad_b))

    print(f"CSV file saved to {CSV_PATH.resolve()}")
